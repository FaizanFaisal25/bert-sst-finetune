# BERT-SST-Finetune

Welcome to the `bert-sst-finetune` repository! This repository focuses on fine-tuning a BERT uncased base model using the Stanford Sentiment Treebank dataset (SST-2) to achieve sentiment analysis.

## Overview

- **Repository URL:** [BERT-SST-Finetune](https://github.com/FaizanFaisal25/bert-sst-finetune.git)
- **Dataset:** [Stanford Sentiment Treebank (SST-2)](https://huggingface.co/datasets/stanfordnlp/sst2)
- **Achieved Accuracy:** 90.6%
- **State-of-the-Art Accuracy:** 97.5% ([Papers with Code](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary))

## Features

- Fine-tunes a BERT uncased base model on SST-2 dataset.
- Achieves a sentiment analysis accuracy of 90.6%.
- Future updates will include cross-comparisons among different models on the SST-2 dataset.

## Getting Started

To view and execute the Jupyter Notebook on Google Colab, access the provided link:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JVQLxpvHryBeJVzlzXF1wudfommfRWMz?usp=sharing)

## Usage

Load the Jupyter Notebook and execute the cells to start fine-tuning the BERT model on the SST-2 dataset. Make sure to set up the correct paths and dependencies as mentioned in the notebook.

## Future Work

- Incorporate cross-comparison analysis amongst various models on SST-2.
- Optimization and tuning to improve the existing accuracy.

## Contributing

Contributions are welcome! Please create an issue to discuss any changes or enhancements.

## References

- [Stanford Sentiment Treebank (SST-2) Dataset](https://huggingface.co/datasets/stanfordnlp/sst2)
- [State-of-the-Art Results on SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary)

Feel free to explore and contribute to the repository. Happy coding!

---

**Author:** Faizan Faisal

[GitHub Profile](https://github.com/FaizanFaisal25)
